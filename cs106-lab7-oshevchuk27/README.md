## CS 106 Lab 7 - Deduplication

Name: Olga Shevchuk

Number of Late Days Using for this lab:
0
---

### Runtime Analysis

See questions in the lab writeup, answer below.
1) I ran three deduplication methods on the SWVF_1_22_med.txt dataset. The allPairsDeduplication() method takes 181.337 seconds to run, 
the hasLinearDeduplication() method takes 0.067 seconds to run and the quickSortDeduplication() method takes 0.184 seconds to run.
2) The graph plotting is also done by reading the SWVF_1_22_med.txt dataset. The plotting might not work for short files as the number of rows to read in was chosen to be larger in order to get a better graph pattern. Therefore, reading from a bigger dataset was necessary for that. As we can see from the resulting graph, the shape of the graph that represents the runtime of allPairsDeduplication() method is a parabola with positive x and y values. The methods runs in quadratic or O(n^2) time. Assuming that n is the number of rows read in from the file and deduplicated, for each of the n objects in the nonduplicated ArrayList, we would have to go through approximately n other objects (for the first one, we would go through the n-1 other objects, for the second one - through n-2 and so on) comparing that object to each of those other objects. That in total contributes O(n^2) runtime which is the slowest and least efficient. The quicksortDeduplication() method produces a graph that represents O(n*log(n)) runtime. First, we perform the in-place quicksort on the existing ArrayList. The runtime of a quickSort algorithm is O(n*log(n)) in the best/ expected case. First, we need to partition the ArrayList. As discussed in class, to estimate the number of levels of partitioning (or the number of partitions) the quicksort performs on the ArrayList, we need decide how many times we need to divide n (the number of object in the unsorted ArrayList) by 2 to get to 1. n/2^x = 1 --> x = log(n) where x is the number of partitions. One partitioning operation has to look at every item in the input to place the pivot at the right position. After the ArrayList is partitioned the first time, the algorithm recursively sorts each of the subArrayLists. Each of those recursive calls looks again at each element in those small subArrayLists but when we add all the elements that get visited per level, it will still be n. Thus each partitioning operation takes O(n) operations, and so the total complexity of quicksort is O(n*log(n)). After that, we walk through the elements of the sorted ArrayList again to compare them, so that takes O(n). However, the total complexity of the quicksortDeduplication() then is O(n) + O(n*log(n)) â‰ˆ O(n*log(n)). However, hashLinearDeduplication() method is the most efficient. It runs in linear, O(n) time. Assuming that n represents the number of objects in the nonduplicated ArrayList, for each one of those elements, we will use get() and put() functions to check if the value associated with that element's key is in the hash map, and if not, create an entry for that element in the hash map. Both of these methods take O(1) time because they rely on locate() or findSlot() functions which are constant and in turn rely on the the constant runtime of several methods of the array that is used to implement the hash map. Thus, the overall runtime of hashLinearDeduplication() is O(n) and that method is the fastest and most efficient because it takes up less energy. 

Project Part:

3) For the main part of the lab, I compared the voters based on only the first and last names (objects with the same first and last names are duplicates). I ran the code with this equality method on all of the short and medium files. The number of duplicates that I received was: 1_22_short : 90; 1_22_med : 5211; 23_44_short : 93 ; 23_44_med : 5341; 45_66_short : 81 ; 45_66_med : 5277; 67_88_short : 104; 67_88_med : 5510. For the project part, I explored other types of equality. The general pattern that I observed is the more attributes I checked, the less duplicates I got. One of the other types of equality I explored was comparing based on last name, first name and middle name. The number of duplicates was significantly smaller compared to the first method.  The number of duplicates that I received was: 1_22_short : 4; 1_22_med : 434; 23_44_short : 7 ; 23_44_med : 332; 45_66_short :2; 45_66_med : 342; 67_88_short : 5; 67_88_med : 389. After that I determined equality based on last, first, middle names and date of birth. The number of duplicates that I received was: 1_22_short : 0; 1_22_med : 1; 23_44_short : 0 ; 23_44_med : 0; 45_66_short :0; 45_66_med : 1; 67_88_short : 0; 67_88_med : 0. Then I considered a type of equality based on last, first, middle names, date of birth and residential address. The number of duplicates that I received was: 1_22_short : 0; 1_22_med : 1; 23_44_short : 0 ; 23_44_med : 0; 45_66_short :0; 45_66_med : 0; 67_88_short : 0; 67_88_med : 0. Finally, I realized that I can get 0 duplicates for each file when comparing only based on voterID. That means that each voter has a different unique ID. However, voterID is not something that is being checked all the time and people might not have this information. That is why I considered another type of equality that would get me 0 duplicates for each file. The smallest sequence of attributes I had to check to get 0 duplicates was comparison based on last name, first name, date of birth, residential address and registration date. Using this equality method will be an efficient way to check for duplicates since it does not have too many attributes and can provide sufficient information to distinguish between the voters.

4) The runtime of the deduplication method using the quicksort algorithm implemented by me is 0.196 seconds when run on SWVF_1_22_med.txt dataset. The runtime of the deduplication method using the built-in sort is 0.169 seconds. When run on the same dataset, the runtime of quickSort() is 0.187 seconds and Collections.sort() : 0.164 seconds. Both quicksort and Collections.sort() have theoretical runtime O(n*log(n)), so they are roughly the same. However, since Collections.sort() is a built-in method, it might be slightly enhanced and work a little bit better than quicksort in certain situations. The reason behind this might be that Collections.sort() is implemented using merge sort algorithm which has a guaranteed n*log(n) runtime while quicksort might be worse in the worst case. We can also see from the plot that the obtained graph for builtinSortDeduplication () method is O(nlog(n)) but is slghtly closer to linear (O(n)) rather than quicksortDeduplication().



### Lab Questionnaire

(None of your answers below will affect your grade; this is to help refine lab
assignments in the future)

1. Approximately, how many hours did you take to complete this lab? (provide
  your answer as a single integer on the line below) 25

2. How difficult did you find this lab? (1-5, with 5 being very difficult and 1
  being very easy) 4

3. Describe the biggest challenge you faced on this lab: The biggest issue I faced during this lab was figuring out ways of how to implement data deduplication methods without removing the duplicates from the existing list but instead by not adding the duplicates to new list. Also, it took 
me lots of time to adjust the number of rows read in to produce graphs that show patterns well.
